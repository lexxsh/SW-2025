"""
Paragraph-level style-transfer & augmentation
--------------------------------------------
* ì…ë ¥ CSV  : title, paragraph_index, paragraphs, generated
* ì¶œë ¥ CSV 1: ì „ì²´ ë°ì´í„°(ë°”ë€ í–‰ì€ generated=1)  â†’ train_generated_llama_3_1_8B_0k.csv
* ì¶œë ¥ CSV 2: ìƒˆë¡œ ìƒì„±ëœ í–‰ë§Œ               â†’ generated_only_llama_3_1_8B_0k.csv
* ëª¨ë¸      : ë¡œì»¬ ê²½ë¡œ /raid/HZ/HZ-sw/llama (decoder-only, left-padding)
"""

import os, re, random, logging, warnings
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm 

MODEL_PATH = "SEOKDONG/llama3.1_korean_v1.1_sft_by_aidx"
BATCH_SIZE = 32
INPUT_CSV = "./data/train_paragraphs.csv"
NUM_SAMPLES = 5  # 450000
OUT_FULL = f"./data/train_llama.csv"
OUT_CHANGED = f"train_generated_only_llama_3_1_8B_{NUM_SAMPLES//1000}k.csv"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


PROMPTS = {
    "narrative_essay": """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ê°ìˆ˜ì„±ì´ í’ë¶€í•œ ì‘ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸€ì„ ë°”íƒ•ìœ¼ë¡œ, ê°œì¸ì ì¸ ê²½í—˜ì´ë‚˜ ê°ì •ì„ ë…¹ì—¬ë‚´ì–´ ë…ìì˜ ë§ˆìŒì— ìš¸ë¦¼ì„ ì£¼ëŠ” ë¶€ë“œëŸ½ê³  ì„œì •ì ì¸ 'ìˆ˜í•„' í˜•ì‹ìœ¼ë¡œ ë¬¸ì²´ë¥¼ ë°”ê¿”ì£¼ì„¸ìš”. ì›ë¬¸ì˜ í•µì‹¬ ì •ë³´ëŠ” ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤. ë¶„ëŸ‰ë„ ìœ ì§€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

**[ì¤‘ìš”] ì˜¤ì§ ë³€í™˜ëœ ë³¸ë¬¸ë§Œ ë‹µë³€í•˜ê³ , ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì„¤ëª…ì´ë‚˜ "(ìµœì¢… ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.)" ì™€ ê°™ì€ ë¶€ê°€ì ì¸ ì½”ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë˜í•œ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”.**

{paragraph}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    "logical_essay": """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì¸ ì¹¼ëŸ¼ë‹ˆìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸€ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ëª…í™•í•œ ì£¼ì¥ì´ë‚˜ ì˜ê²¬ì„ ì„¤ë“ë ¥ ìˆê²Œ ì „ë‹¬í•˜ëŠ” ì§€ì ì¸ 'ì—ì„¸ì´' í˜•ì‹ìœ¼ë¡œ ë¬¸ì²´ë¥¼ ë°”ê¿”ì£¼ì„¸ìš”. ë¶„ëŸ‰ë„ ìœ ì§€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

**[ì¤‘ìš”] ì˜¤ì§ ë³€í™˜ëœ ë³¸ë¬¸ë§Œ ë‹µë³€í•˜ê³ , ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì„¤ëª…ì´ë‚˜ "(ìµœì¢… ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.)" ì™€ ê°™ì€ ë¶€ê°€ì ì¸ ì½”ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë˜í•œ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”.**

{paragraph}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    "civic_essay": """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ì„¤ëª…ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸€ì˜ ë‚´ìš©ì„ ë…ìë“¤ì´ ì•Œì•„ë“¯ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì„¤ëª…ì²´ + í™•ì¸í˜• ì¢…ê²°ì–´ë¯¸ ì¡°í•©ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë¶„ëŸ‰ë„ ìœ ì§€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

**[ì¤‘ìš”] ì˜¤ì§ ë³€í™˜ëœ ë³¸ë¬¸ë§Œ ë‹µë³€í•˜ê³ , ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì„¤ëª…ì´ë‚˜ "(ìµœì¢… ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.)" ì™€ ê°™ì€ ë¶€ê°€ì ì¸ ì½”ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë˜í•œ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”.**

{paragraph}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    "news": """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ëƒ‰ì² í•œ ê¸°ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸€ì—ì„œ ê°ì •ì ì¸ í‘œí˜„ì€ ëª¨ë‘ ë°°ì œí•˜ê³ , ê°ê´€ì ì¸ ì‚¬ì‹¤ê³¼ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ìœ¡í•˜ì›ì¹™ì— ë”°ë¼ ê°„ê²°í•˜ê³  ëª…ë£Œí•œ 'ë‰´ìŠ¤ ê¸°ì‚¬' í˜•ì‹ìœ¼ë¡œ ë¬¸ì²´ë¥¼ ë°”ê¿”ì£¼ì„¸ìš”.  ë¶„ëŸ‰ë„ ìœ ì§€í•´ì£¼ì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

**[ì¤‘ìš”] ì˜¤ì§ ë³€í™˜ëœ ë³¸ë¬¸ë§Œ ë‹µë³€í•˜ê³ , ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì„¤ëª…ì´ë‚˜ "(ìµœì¢… ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.)" ì™€ ê°™ì€ ë¶€ê°€ì ì¸ ì½”ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë˜í•œ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”.**

{paragraph}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    "summary": """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸€ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ 'ìš”ì•½'í•´ì£¼ì„¸ìš”. ë‹¨, ì›ë˜ ê¸€ì˜ ë¬¸ì²´ì™€ í†¤ì€ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤. ìŠ¤íƒ€ì¼ì„ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.<|eot_id|><|start_header_id|>user<|end_header_id|>

**[ì¤‘ìš”] ì˜¤ì§ ë³€í™˜ëœ ë³¸ë¬¸ë§Œ ë‹µë³€í•˜ê³ , ê´„í˜¸ë¥¼ ì‚¬ìš©í•œ ì„¤ëª…ì´ë‚˜ "(ìµœì¢… ë²„ì „ì„ ì œê³µí•©ë‹ˆë‹¤.)" ì™€ ê°™ì€ ë¶€ê°€ì ì¸ ì½”ë©˜íŠ¸ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. ë˜í•œ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”.**

{paragraph}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
}
STYLE_LIST = list(PROMPTS.keys())

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO)
log = logging.getLogger("style-aug")
warnings.filterwarnings("ignore")

log.info(f"ğŸ¤” ëª¨ë¸ ë¡œë”©: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH, use_fast=False, padding_side="left"
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model = (
    AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16)
    .to(DEVICE)
    .eval()
)

torch.backends.cuda.matmul.allow_tf32 = True
model = torch.compile(model, mode="reduce-overhead", fullgraph=True)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def generate_batch(prompts, max_tokens=256, temperature=0.7, top_p=0.95):
#     """HF ëª¨ë¸ ë°°ì¹˜ ìƒì„± â†’ list[str]"""
#     enc = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(
#         DEVICE
#     )
#     with torch.no_grad():
#         out = model.generate(
#             **enc,
#             max_new_tokens=max_tokens,
#             do_sample=True,
#             temperature=temperature,
#             top_p=top_p,
#             eos_token_id=tokenizer.eos_token_id,
#             pad_token_id=tokenizer.eos_token_id,
#         )
#     prompt_len = enc["input_ids"].shape[1]
#     txt = tokenizer.batch_decode(out[:, prompt_len:], skip_special_tokens=True)
#     return [re.split(r"<\|eot_id\|>|<\|end_of_text\|>", t)[0].strip() for t in txt]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ vLLM ì „ìš© ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€
from vllm import LLM, SamplingParams

llm = LLM(
    model=MODEL_PATH,  # /raid/HZ/HZ-sw/llama
    tokenizer=MODEL_PATH,
    dtype="float16",
    tensor_parallel_size=1,  # ë‹¤ì¤‘ GPUë©´ >1
    gpu_memory_utilization=0.90,  # OOM ë°©ì§€ìš©
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ vLLM ë²„ì „ generate_batch â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_batch(prompts, max_tokens=256, temperature=0.7, top_p=0.95):
    """
    vLLM ë°°ì¹˜ ìƒì„± â†’ list[str]
    """
    sampling_params = SamplingParams(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stop=["<|eot_id|>", "<|end_of_text|>"],  # ëª¨ë¸ íŠ¹ìˆ˜í† í°
    )

    # vLLMì€ ì…ë ¥ ìˆœì„œë¥¼ ë³´ì¥í•˜ë ¤ë©´ request_id ì‚¬ìš© or ì •ë ¬ í•„ìš”
    outs = llm.generate(prompts, sampling_params)
    # RequestOutput.idëŠ” 0,1,2,â€¦ ìˆœìœ¼ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ ì •ë ¬ í›„ ì¶”ì¶œ
    outs_sorted = sorted(outs, key=lambda o: o.request_id)
    return [o.outputs[0].text.strip() for o in outs_sorted]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ë°ì´í„° ì½ê¸° & ìƒ˜í”Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log.info(f"ğŸ“„ CSV ë¡œë“œ: {INPUT_CSV}")
df = pd.read_csv(INPUT_CSV)
if "generated" not in df.columns:
    df["generated"] = 0

human_df = df[df["generated"] == 0]
sample_df = (
    human_df.sample(n=NUM_SAMPLES, random_state=42)
    if len(human_df) >= NUM_SAMPLES
    else human_df
).reset_index()
log.info(f"ğŸ§‘ ëŒ€ìƒ í–‰: {len(sample_df)} / ì „ì²´ {len(df)}")

split_idx = np.array_split(sample_df.index, len(STYLE_LIST))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ìŠ¤íƒ€ì¼ë³„ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
changed_rows = []

for s_idx, style in enumerate(STYLE_LIST):
    log.info(f"\nğŸ”¥ [{style.upper()}] ({s_idx+1}/5)")
    rows_slice = sample_df.loc[split_idx[s_idx]]
    if rows_slice.empty:
        log.info("ëŒ€ìƒ ì—†ìŒ, ìŠ¤í‚µ")
        continue

    prompts = [
        PROMPTS[style].format(paragraph=r["paragraphs"])
        for _, r in rows_slice.iterrows()
    ]
    log.info(f"ğŸš€ í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ â†’ LLM")

    generated = []
    # tqdm â”€ ìƒì„± ë°°ì¹˜ ì§„í–‰ë¥  í‘œì‹œ
    for st in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f"{style} gen"):
        generated += generate_batch(prompts[st : st + BATCH_SIZE])

    # ê²°ê³¼ ë°˜ì˜ â†’ tqdm ìƒíƒœë°”
    for (_, orig), gen_text in tqdm(
        zip(rows_slice.iterrows(), generated),
        total=len(rows_slice),
        desc=f"{style} apply",
        leave=False,
    ):
        row_idx = orig["index"]
        df.at[row_idx, "paragraphs"] = gen_text
        df.at[row_idx, "generated"] = 1
        changed_rows.append(df.loc[[row_idx]])

    log.info("\nğŸ’¾ ì„ì‹œ CSV ì €ì¥")
    if changed_rows:
        pd.concat(changed_rows).to_csv(OUT_CHANGED, index=False, encoding="utf-8-sig")
        log.info(f"ì„ì‹œ ë³€í™˜ í–‰ {len(changed_rows)}ê°œ â†’ {OUT_CHANGED}")
    else:
        log.info("ì„ì‹œ ë³€í™˜ëœ í–‰ ì—†ìŒ")

    df.to_csv(OUT_FULL, index=False, encoding="utf-8-sig")
    log.info(f"ì„ì‹œ íŒŒì¼ â†’ {OUT_FULL}")
    log.info("âœ… ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log.info("\nğŸ’¾ CSV ì €ì¥")
df.to_csv(OUT_FULL, index=False, encoding="utf-8-sig")
log.info(f"ì „ì²´ íŒŒì¼ â†’ {OUT_FULL}")

if changed_rows:
    pd.concat(changed_rows).to_csv(OUT_CHANGED, index=False, encoding="utf-8-sig")
    log.info(f"ë³€í™˜ í–‰ {len(changed_rows)}ê°œ â†’ {OUT_CHANGED}")
else:
    log.info("ë³€í™˜ëœ í–‰ ì—†ìŒ")