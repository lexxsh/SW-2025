import os
import torch
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
import torch.nn as nn
from tqdm import tqdm
import random
import numpy as np

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


set_seed(42)

model_name = "team-lucid/deberta-v3-base-korean"
train_csv_path = "./data/train.csv"
batch_size = 16
lr = 2e-5
epochs = 3
max_length = 512
window_size = 400 
overlap = 100

device_num = 0
device = torch.device(f"cuda:{device_num}" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

def sliding_window_split(text, tokenizer, window_size=400, overlap=100):
    """í…ìŠ¤íŠ¸ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë¶„í• """
    tokens = tokenizer.tokenize(text)

    if len(tokens) <= window_size:
        return [text]

    windows = []
    start = 0

    while start < len(tokens):
        end = min(start + window_size, len(tokens))
        window_tokens = tokens[start:end]
        window_text = tokenizer.convert_tokens_to_string(window_tokens)
        windows.append(window_text)

        if end >= len(tokens):
            break
        start += window_size - overlap

    return windows

class SlidingWindowDataset(Dataset):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•™ìŠµìš© ë°ì´í„°ì…‹"""

    def __init__(self, df: pd.DataFrame, tokenizer, window_size=400, overlap=100):
        self.windows = []
        self.labels = []

        print("ğŸ”„ Creating sliding windows...")
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Processing documents"):
            text = str(row["full_text"])
            label = int(row["generated"])

            windows = sliding_window_split(text, tokenizer, window_size, overlap)
            self.windows.extend(windows)
            self.labels.extend([label] * len(windows))

        self.tokenizer = tokenizer
        print(f"âœ… Total windows created: {len(self.windows)}")

        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        unique, counts = np.unique(self.labels, return_counts=True)
        for u, c in zip(unique, counts):
            print(f"   Label {u}: {c:,} windows")

    def __len__(self):
        return len(self.windows)

    def __getitem__(self, idx):
        text = self.windows[idx]
        tokens = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding="max_length",
            max_length=max_length,
        )
        input_ids = tokens.input_ids.squeeze(0)
        attention_mask = tokens.attention_mask.squeeze(0)
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return input_ids, attention_mask, label

class SimpleClassifier(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        d = self.backbone.config.hidden_size
        self.norm = nn.LayerNorm(d)
        self.drop = nn.Dropout(0.2)
        self.fc = nn.Linear(d, 1)

    def forward(self, input_ids, attention_mask):
        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = out.last_hidden_state[:, 0, :]
        h = self.norm(cls_token)
        h = self.drop(h)
        return self.fc(h).squeeze(-1)

def run_epoch(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0

    for input_ids, attention_mask, label in tqdm(dataloader, desc="Training"):
        input_ids, attention_mask, label = [
            t.to(device) for t in (input_ids, attention_mask, label)
        ]

        optimizer.zero_grad()
        logits = model(input_ids, attention_mask)
        loss = criterion(logits, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # ì •í™•ë„ ê³„ì‚°
        predictions = (torch.sigmoid(logits) > 0.5).float()
        correct_predictions += (predictions == label).sum().item()
        total_predictions += label.size(0)

    avg_loss = total_loss / len(dataloader)
    accuracy = correct_predictions / total_predictions

    return avg_loss, accuracy

def train_sliding_window():
    print("ğŸš€ RoBERTa-Base ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•™ìŠµ ì‹œì‘!")

    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(train_csv_path).dropna(subset=["full_text"])
    print(f"âœ… Loaded {len(df)} documents")
    print("Document class distribution:")
    print(df["generated"].value_counts())

    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ë°ì´í„°ì…‹ ìƒì„±
    train_ds = SlidingWindowDataset(df, tokenizer, window_size, overlap)

    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìƒ˜í”Œë§ (ì„ íƒì‚¬í•­)
    max_windows = 3000000000  # ìµœëŒ€ ìœˆë„ìš° ìˆ˜ ì œí•œ
    if len(train_ds) > max_windows:
        print(f"âš ï¸  Too many windows ({len(train_ds)}), sampling to {max_windows}")
        indices = random.sample(range(len(train_ds)), max_windows)
        train_ds.windows = [train_ds.windows[i] for i in indices]
        train_ds.labels = [train_ds.labels[i] for i in indices]
        print(f"âœ… Sampled to {len(train_ds)} windows")

    # DataLoader
    train_dl = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2
    )

    # ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤í•¨ìˆ˜
    model = SimpleClassifier(model_name).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCEWithLogitsLoss()

    # ì €ì¥ ë””ë ‰í† ë¦¬
    save_dir = "./ckpt/full_text_sliding_window"
    os.makedirs(save_dir, exist_ok=True)

    print(f"ğŸ”¥ Training started with {len(train_ds)} windows")
    print(f"   Batch size: {batch_size}")
    print(f"   Learning rate: {lr}")
    print(f"   Epochs: {epochs}")

    # í•™ìŠµ
    best_accuracy = 0.0
    for ep in range(1, epochs + 1):
        print(f"\nğŸ“ Epoch {ep}/{epochs}")

        train_loss, train_acc = run_epoch(model, train_dl, optimizer, criterion)

        print(f"[Epoch {ep}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ckpt_path = os.path.join(save_dir, f"epoch_{ep}.pt")
        torch.save(
            {
                "epoch": ep,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "train_loss": train_loss,
                "train_accuracy": train_acc,
                "config": {
                    "model_name": model_name,
                    "window_size": window_size,
                    "overlap": overlap,
                    "lr": lr,
                    "batch_size": batch_size,
                },
            },
            ckpt_path,
        )
        print(f"âœ… Checkpoint saved â†’ {ckpt_path}")

        if train_acc > best_accuracy:
            best_accuracy = train_acc
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë³„ë„ ì €ì¥
            best_path = os.path.join(save_dir, "best_model.pt")
            torch.save(
                {
                    "epoch": ep,
                    "model_state_dict": model.state_dict(),
                    "train_accuracy": train_acc,
                    "train_loss": train_loss,
                },
                best_path,
            )
            print(f"ğŸ¯ New best accuracy: {best_accuracy:.4f} â†’ {best_path}")

    print(f"\nğŸ‰ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {save_dir}")
    print(f"ğŸ† Best accuracy: {best_accuracy:.4f}")
    print(f"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ìˆ˜ë„ ë¼ë²¨ë§ì„ ìœ„í•´ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”")

    return save_dir


# ================= ì‹¤í–‰ =================
if __name__ == "__main__":
    model_dir = train_sliding_window()
    print(f"\nâœ¨ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ìœ„ì¹˜: {model_dir}")