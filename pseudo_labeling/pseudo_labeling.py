import pandas as pd
import re
import os, torch, numpy as np, pandas as pd, logging, warnings
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import argparse
from tqdm import tqdm

parser = argparse.ArgumentParser()
parser.add_argument(
    "--model_path",
    default="./ckpt/full_text_sliding_window/epoch_1.pt",
    help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸(.pt) ê²½ë¡œ",
)
parser.add_argument(
    "--train_csv", default="./data/train.csv", help="ì›ë³¸ í•™ìŠµ ë°ì´í„°ì…‹"
)
parser.add_argument(
    "--output_csv",
    default="./data/train_pseudo_label.csv",
    help="ê²°ê³¼ ì €ìž¥ ê²½ë¡œ",
)
args = parser.parse_args()

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_NAME = "team-lucid/deberta-v3-base-korean"
MAX_LEN, BATCH_SIZE = 512, 4

# â”€â”€â”€â”€â”€ 1. CSV ë¶ˆëŸ¬ì˜¤ê¸° â”€â”€â”€â”€â”€
df = pd.read_csv("./data/train.csv")


# â”€â”€â”€â”€â”€ 2. ë¬¸ë‹¨ ë¶„ë¦¬ í•¨ìˆ˜ ìˆ˜ì • â”€â”€â”€â”€â”€
def split_into_paragraphs(text):
    # í•œ ì¤„ ê°œí–‰ ê¸°ì¤€ìœ¼ë¡œ ìžë¥´ë˜, ì™„ì „ížˆ ë¹ˆ ì¤„ì€ ë¬´ì‹œ
    return [p.strip() for p in text.split("\n") if p.strip()]


# â”€â”€â”€â”€â”€ 3. í–‰ ë‹¨ìœ„ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬ ë° ìƒˆ DataFrame ìƒì„± â”€â”€â”€â”€â”€
rows = []
for _, row in df.iterrows():
    paragraphs = split_into_paragraphs(row["full_text"])
    for idx, para in enumerate(paragraphs):
        rows.append(
            {
                "title": row["title"],
                "paragraph_index": idx,
                "paragraph_text": para,
                "generated": row["generated"],  # â¬…ï¸ generated ê°’ ìœ ì§€
            }
        )

# â”€â”€â”€â”€â”€ 4. ìƒˆ DataFrame ì €ìž¥ â”€â”€â”€â”€â”€
paragraph_df = pd.DataFrame(rows)
paragraph_df_0 = paragraph_df[paragraph_df["generated"] == 0]
paragraph_df_1 = paragraph_df[paragraph_df["generated"] == 1]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ëª¨ë¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SimpleClassifier(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = self.backbone.config.hidden_size
        self.norm = nn.LayerNorm(hidden_size)
        self.drop = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]
        h = self.norm(cls_token)
        h = self.drop(h)
        return self.fc(h).squeeze(-1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_df = paragraph_df_1
test_sents = (test_df["title"] + " " + test_df["paragraph_text"]).tolist()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


class TestDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }


test_dataset = TestDataset(test_sents, tokenizer)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = SimpleClassifier(MODEL_NAME).to(DEVICE)
ckpt = torch.load(args.model_path, map_location=DEVICE)
model.load_state_dict(ckpt["model_state_dict"])
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ì¶”ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_probs = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc="ðŸ” Inference"):
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.extend(probs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ì œì¶œ íŒŒì¼ ì €ìž¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_df["generated"] = all_probs
paragraph_df_1["generated"] = (test_df["generated"] > 0.5).astype(int)

# â”€â”€â”€â”€â”€ 3. ë³‘í•© â”€â”€â”€â”€â”€
merged_df = pd.concat([paragraph_df_0, paragraph_df_1], ignore_index=True)
output_path = args.output_csv
merged_df.to_csv(output_path, index=False, encoding="utf-8-sig")
print(f"âœ… ê²°ê³¼ ì €ìž¥ ì™„ë£Œ â†’ {output_path}")
