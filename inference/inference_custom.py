#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
inference.py â€” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ í•™ìŠµ ëª¨ë¸(SimpleClassifier)ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ì¶”ë¡ 
"""

import os, torch, numpy as np, pandas as pd, logging, warnings
from transformers import AutoTokenizer, AutoModel
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import argparse
from tqdm import tqdm

# safetensors ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ë‹¤ë©´ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤: pip install safetensors
from safetensors.torch import load_file

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. Argument íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser()
# --model_path ì¸ìì— .pt íŒŒì¼ì´ ì•„ë‹Œ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
parser.add_argument(
    "--model_path",
    default="/home/elicer/ckpt_sota_with_hz_think/checkpoint-24818",
    help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œ",
)
parser.add_argument(
    "--output_csv", default="./submission/submission_custom.csv", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
)
args = parser.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_NAME = "team-lucid/deberta-v3-base-korean"
MAX_LEN, BATCH_SIZE = 512, 4


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ëª¨ë¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SimpleClassifier(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = self.backbone.config.hidden_size
        self.norm = nn.LayerNorm(hidden_size)
        self.drop = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]
        h = self.norm(cls_token)
        h = self.drop(h)
        return self.fc(h).squeeze(-1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_df = pd.read_csv("./data/test.csv", encoding="utf-8-sig")
test_sents = (test_df["title"] + " " + test_df["paragraph_text"]).tolist()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í† í¬ë‚˜ì´ì €ëŠ” ì²´í¬í¬ì¸íŠ¸ í´ë”ê°€ ì•„ë‹Œ ì›ë˜ ëª¨ë¸ì˜ ê²ƒì„ ì‚¬ìš©í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
# ë§Œì•½ ì²´í¬í¬ì¸íŠ¸ì— ì»¤ìŠ¤í…€ í† í¬ë‚˜ì´ì €ê°€ ìˆë‹¤ë©´ args.model_pathë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


class TestDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }


test_dataset = TestDataset(test_sents, tokenizer)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ëª¨ë¸ ë¡œë”© (ìˆ˜ì •ëœ ë¶€ë¶„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¨¼ì € ëª¨ë¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
model = SimpleClassifier(MODEL_NAME).to(DEVICE)

# ì²´í¬í¬ì¸íŠ¸ í´ë” ë‚´ì˜ ì‹¤ì œ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
# ì¼ë°˜ì ìœ¼ë¡œ 'model.safetensors' ë˜ëŠ” 'pytorch_model.bin' ì…ë‹ˆë‹¤.
weights_path = os.path.join(args.model_path, "model.safetensors")
if not os.path.exists(weights_path):
    weights_path = os.path.join(args.model_path, "pytorch_model.bin")

# ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ state_dictë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
if weights_path.endswith(".bin"):
    state_dict = torch.load(weights_path, map_location=DEVICE)
else:
    # .safetensors íŒŒì¼ ë¡œë“œ
    state_dict = load_file(weights_path, device=str(DEVICE))

# ëª¨ë¸ì— state_dictë¥¼ ì ìš©í•©ë‹ˆë‹¤.
model.load_state_dict(state_dict)
model.eval()
print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {weights_path}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ì¶”ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_probs = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc="ğŸ” Inference"):
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.extend(probs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ì œì¶œ íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sub = pd.read_csv("./data/sample_submission.csv", encoding="utf-8-sig")
sub["generated"] = all_probs
output_path = f"{args.output_csv}"
sub.to_csv(output_path, index=False, encoding="utf-8-sig")
print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}")
