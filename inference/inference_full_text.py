"""
inference.py â€” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ í•™ìŠµ ëª¨ë¸(SimpleClassifier)ë¡œ í…ŒìŠ¤íŠ¸ì…‹ ì¶”ë¡ 
"""

import os, torch, numpy as np, pandas as pd, logging, warnings
from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import argparse
from tqdm import tqdm

parser = argparse.ArgumentParser()
parser.add_argument("--model_path", required=True, help="ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸(.pt) ê²½ë¡œ")
parser.add_argument("--test_csv", default="./data/test.csv", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
parser.add_argument("--output_csv", default="./submission/submission_full_text.csv", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
args = parser.parse_args()

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_NAME = "team-lucid/deberta-v3-base-korean"
MAX_LEN, BATCH_SIZE = 512, 4


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ëª¨ë¸ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SimpleClassifier(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        hidden_size = self.backbone.config.hidden_size
        self.norm = nn.LayerNorm(hidden_size)
        self.drop = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]
        h = self.norm(cls_token)
        h = self.drop(h)
        return self.fc(h).squeeze(-1)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë°ì´í„° ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_df = pd.read_csv(args.test_csv, encoding="utf-8-sig")
test_sents = (test_df["title"] + " " + test_df["paragraph_text"]).tolist()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


class TestDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
        }


test_dataset = TestDataset(test_sents, tokenizer)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = SimpleClassifier(MODEL_NAME).to(DEVICE)
ckpt = torch.load(args.model_path, map_location=DEVICE)
model.load_state_dict(ckpt["model_state_dict"])
model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ì¶”ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_probs = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc="ğŸ” Inference"):
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        logits = model(input_ids, attention_mask)
        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.extend(probs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ì œì¶œ íŒŒì¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sub = pd.read_csv(
    "./data/sample_submission.csv", encoding="utf-8-sig"
)
sub["generated"] = all_probs
output_path = f"./submission/{args.output_csv}"
sub.to_csv(output_path, index=False, encoding="utf-8-sig")
print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}")